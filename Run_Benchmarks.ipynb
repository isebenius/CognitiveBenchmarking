{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import sys\n",
    "import dill\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import symusic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark_generators import CadenceGenerator, IntervalLikelihoodGenerator, \\\n",
    "                                IntervalRecognitionGenerator, MelodyContinuationGenerator, \\\n",
    "                                ScaleFillingGenerator, BarShufflingGenerator, TrackTranspositionGenerator, \\\n",
    "                                RandomNoteAlterationGenerator\n",
    "\n",
    "from utils import time_dilate_midi, transpose_midi\n",
    "#from utils import reshuffle_bars, transpose_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from amads.expectation.tokenizer import MelodyIntervalTokenizer, IOITokenizer, Token, REMITokenizer, TSDTokenizer, TSDTokenizer_Custom\n",
    "from amads.expectation.dataset import ScoreDataset\n",
    "from amads.expectation.model import LSTM\n",
    "from amads.expectation.metrics import NegativeLogLikelihood, Entropy, InformationContent\n",
    "from amads.core.basics import Note, Score\n",
    "from scipy import stats\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # INTERVAL PREDICTION\n",
    "\n",
    "# intervals = ['-octave',\n",
    "#  '-maj7',\n",
    "#  '-min7',\n",
    "#  '-maj6',\n",
    "#  '-min6',\n",
    "#  '-p5',\n",
    "#  '-tritone',\n",
    "#  '-p4',\n",
    "#  '-maj3',\n",
    "#  '-min3',\n",
    "#  '-maj2',\n",
    "#  '-min2',\n",
    "#  'unison',\n",
    "#  'min2',\n",
    "#  'maj2',\n",
    "#  'min3',\n",
    "#  'maj3',\n",
    "#  'p4',\n",
    "#  'tritone',\n",
    "#  'p5',\n",
    "#  'min6',\n",
    "#  'maj6',\n",
    "#  'min7',\n",
    "#  'maj7',\n",
    "#  'octave']\n",
    "\n",
    "# n_perm = 5\n",
    "\n",
    "# for i in range(n_perm):\n",
    "\n",
    "#     generator = IntervalRecognitionGenerator(output_dir = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/IDyOM_data/interval_recognition_'+str(i + 1)+'/')\n",
    "\n",
    "#     for interval in intervals:\n",
    "\n",
    "#         tests = generator.generate_comprehensive_test(interval, num_examples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_BENCHMARK_SUITE = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = torch.load('../lstm_atepp_not_bad_quality_embedd23_hidden64_300epochs_batch4_lr0.01_tokenizer_fixed.pth', map_location='cpu')\n",
    "lstm.eval()\n",
    "lstm.device = 'cpu'\n",
    "model = lstm\n",
    "\n",
    "custom_params = {'time_range' : (0.01, 1), 'time_factor': 1}\n",
    "tokenizer = TSDTokenizer_Custom(config_params = custom_params)\n",
    "\n",
    "# lstm_small = torch.load('../lstm_model_test_3.pth')\n",
    "# lstm_small.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_nll(midi_path) -> float:\n",
    "\n",
    "    # Tokenize both MIDI files\n",
    "    tokens = tokenizer.tokenize(midi_path)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict_sequence(tokens)\n",
    "    \n",
    "    # Calculate mean NLLs (ignoring None values)\n",
    "    nlls = [nll for nll in predictions.nlls if nll is not None]\n",
    "    \n",
    "    mean_nll = np.mean(nlls)\n",
    "    \n",
    "    # Return results\n",
    "    return mean_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_nll(midi_path) -> float:\n",
    "\n",
    "    # Tokenize both MIDI files\n",
    "    tokens = tokenizer.tokenize(midi_path)\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict_sequence(tokens)\n",
    "    \n",
    "    # Calculate mean NLLs (ignoring None values)\n",
    "    nlls = [nll for nll in predictions.nlls if nll is not None]\n",
    "    \n",
    "    total_nll = np.sum(nlls)\n",
    "    \n",
    "    # Return results\n",
    "    return total_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenizer = TSDTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CADENCE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = CadenceGenerator(output_dir='/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/cadence_examples/')\n",
    "# cadence_resolutions = generator.generate_all_resolutions(key_name='C', backbone_progression = [1,4,1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cadence_prediction_benchmark(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results = False):\n",
    "    \n",
    "    '''\n",
    "    Runs cadence prediction benchmark.\n",
    "    Arguments:\n",
    "    get_mean_nll: callable, str -> float, takes in midi file and outputs mean NLL\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory\n",
    "    return_all_results: if True, return the percentile of the tonic key for every studied key. If False, just return the mean. Default=False\n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of percetile of tonic cadence for each of 12 tonic keys (C4-B5).\n",
    "    mean_percentile: Average percentile of tonic across all keys. Higher = better.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_maj = []\n",
    "    all_min = []\n",
    "    all_dim = []\n",
    "\n",
    "    for i in range(12):\n",
    "\n",
    "        maj_nll = []\n",
    "        min_nll = []\n",
    "        dim_nll = []\n",
    "\n",
    "        for j in range(12):\n",
    "\n",
    "            major = PATH_TO_BENCHMARK_SUITE + 'cadence/' + str(60+i)+'_from_I-IV-I-V_to_' + str(j)+'_maj.mid'\n",
    "            minor = PATH_TO_BENCHMARK_SUITE + 'cadence/' + str(60+i)+'_from_I-IV-I-V_to_' + str(j)+'_min.mid'\n",
    "            dimin = PATH_TO_BENCHMARK_SUITE + 'cadence/' + str(60+i)+'_from_I-IV-I-V_to_' + str(j)+'_dim.mid'\n",
    "\n",
    "            maj_nll.append(get_mean_nll(major))\n",
    "            min_nll.append(get_mean_nll(minor))\n",
    "            dim_nll.append(get_mean_nll(dimin)) \n",
    "\n",
    "        all_maj.append(maj_nll)\n",
    "        all_min.append(min_nll)\n",
    "        all_dim.append(dim_nll)\n",
    "        \n",
    "    all_maj = pd.DataFrame(all_maj, columns = [str(x) + '_maj' for x in range(0,12)]).T\n",
    "    all_min = pd.DataFrame(all_min, columns = [str(x) + '_min' for x in range(0,12)]).T\n",
    "    all_dim = pd.DataFrame(all_dim, columns = [str(x) + '_dim' for x in range(0,12)]).T\n",
    "    \n",
    "    all_results = pd.concat([all_maj, all_min, all_dim])\n",
    "    all_results = (-all_results).rank(0, pct = True).T[['0_maj']].rename(columns = {'0_maj':'Percentile of tonic'})\n",
    "    mean_percentile = np.mean(all_results)\n",
    "    \n",
    "    if return_all_results == True:\n",
    "        return all_results, mean_percentile\n",
    "    \n",
    "    else:\n",
    "        return mean_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8958333333333334"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cadence_prediction_benchmark(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE FILLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = ScaleFillingGenerator(output_dir=\"/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/scale_filling/\")\n",
    "\n",
    "# c_major_context, control_files = generator.generate_all_scale_variations(\n",
    "#     key_name=\"C\",\n",
    "#     mode='major', include_controls=True\n",
    "# )\n",
    "\n",
    "# for k, fname in enumerate(c_major_context):\n",
    "#     for i in range(-12,13):\n",
    "#         new_fname = str(60+i) + fname.split('/')[-1][1:].split('.')[0] + '.mid'\n",
    "#         new_fname_control = str(60+i) + fname.split('/')[-1][1:].split('.')[0] + '_control.mid'\n",
    "        \n",
    "#         transpose_midi(input_path=fname, output_path=PATH_TO_BENCHMARK_SUITE + 'scale_filling/' + new_fname, semitones = i)\n",
    "#         transpose_midi(input_path=control_files[k], output_path= PATH_TO_BENCHMARK_SUITE + 'scale_filling/' + new_fname_control, semitones = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_filling_dir = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/scale_filling/'\n",
    "files = [scale_filling_dir + x for x in os.listdir(scale_filling_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_file = [x for x in files if ('correct' in x) and ('temp' not in x)][0]\n",
    "incorrect_files = [x for x in files if (x.endswith('mid')) and ('correct' not in x) and ('temp' not in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_BENCHMARK_SUITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scale_filling_benchmark(get_total_nll, PATH_TO_BENCHMARK_SUITE, return_all_results = False):\n",
    "    \n",
    "    '''\n",
    "    Runs scale filling benchmark.\n",
    "    Arguments:\n",
    "    get_total_nll: callable, str -> float, takes in midi file and outputs total NLL\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory\n",
    "    return_all_results: if True, return the percentile of the tonic key for every studied key. If False, just return the mean. Default=False\n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of percetile of tonic cadence for each of 12 tonic keys (C4-B5).\n",
    "    mean_percentile: Average percentile of tonic across all keys. Higher = better.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    all_correct_nlls = []\n",
    "    all_incorrect_nlls = []\n",
    "\n",
    "    for i in range(48,73):\n",
    "\n",
    "        all_files = glob.glob(PATH_TO_BENCHMARK_SUITE + 'scale_filling/'+str(i) + '_*')\n",
    "        correct = [x for x in all_files if (x.endswith('mid')) and ('correct' in x) and ('control' not in x)][0]\n",
    "        correct_control = correct.split('.mid')[0] + '_control.mid'\n",
    "\n",
    "        wrong = [x for x in all_files if (x.endswith('mid')) and ('correct' not in x) and ('control' not in x)]\n",
    "        wrong_control = [x.split('.mid')[0] + '_control.mid' for x in wrong]\n",
    "\n",
    "        #THIS VERSION INCLUDES A NORMALIZATION\n",
    "        nll_correct = get_total_nll(correct) - get_total_nll(correct_control)\n",
    "        nlls_incorrect = [get_total_nll(wrong[k]) - get_total_nll(wrong_control[k]) for k in range(len(wrong))]\n",
    "        #nlls_incorrect = [get_mean_nll(wrong[k]) for k in range(len(wrong))]\n",
    "\n",
    "        all_correct_nlls.append(nll_correct)\n",
    "        all_incorrect_nlls.append(nlls_incorrect)\n",
    "\n",
    "    correct_scale_rank = [np.argsort(np.argsort(all_incorrect_nlls[i] + [all_correct_nlls[i]]))[-1] + 1  for i in range(-12,13)]\n",
    "    correct_scale_rank = pd.DataFrame(correct_scale_rank, columns = ['Correct Scale Rank'])\n",
    "\n",
    "    all_results = pd.DataFrame((((24 - correct_scale_rank) + 1)/24).values, columns = ['Average Percentile'])\n",
    "    mean_percentile = np.mean(all_results)\n",
    "    \n",
    "    if return_all_results == True:\n",
    "        return all_results, mean_percentile\n",
    "    \n",
    "    else:\n",
    "        return mean_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7283333333333334"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_scale_filling_benchmark(get_total_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ScaleFillingGenerator(output_dir=\"/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/scale_filling_examples/\")\n",
    "\n",
    "c_major_context = generator.generate_all_scale_variations(\n",
    "    key_name=\"C\",\n",
    "    mode='major',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12\n",
      "-11\n",
      "-10\n",
      "-9\n",
      "-8\n",
      "-7\n",
      "-6\n",
      "-5\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "scale_filling_dir = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/scale_filling_examples/'\n",
    "files = [scale_filling_dir + x for x in os.listdir(scale_filling_dir)]\n",
    "correct_file = [x for x in files if ('correct' in x) and ('temp' not in x)][0]\n",
    "incorrect_files = [x for x in files if (x.endswith('mid')) and ('correct' not in x) and ('temp' not in x)]\n",
    "\n",
    "all_correct_nlls = []\n",
    "all_incorrect_nlls = []\n",
    "\n",
    "for i in range(-12,13):\n",
    "    print(i)\n",
    "    correct = transpose_midi(input_path=correct_file, output_path='/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/scale_filling_examples/temp_correct.mid', semitones = i)\n",
    "    wrong = [transpose_midi(input_path=x, output_path='/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/scale_filling_examples/temp_wrong'+str(j)+'.mid', semitones = i) for j, x in enumerate(incorrect_files)]\n",
    "    nll_correct = get_mean_nll(model, tokenizer, correct)#compare_midi_sequence_nlls(lstm_small, TSDTokenizer(), correct, wrong[0])['correct_mean_nll']\n",
    "    nlls_incorrect = [get_mean_nll(model, tokenizer, x) for x in wrong]\n",
    "\n",
    "    all_correct_nlls.append(nll_correct)\n",
    "    all_incorrect_nlls.append(nlls_incorrect)\n",
    "    \n",
    "# tokenizer = TSDTokenizer()\n",
    "# correct_midi_path = os.path.listdir(scale_filling_dir+'*correct*')\n",
    "# correct_tokens = tokenizer.tokenize(correct_midi_path)\n",
    "# incorrect_tokens = tokenizer.tokenize(incorrect_midi_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_scale_rank = [np.argsort(np.argsort(all_incorrect_nlls[i] + [all_correct_nlls[i]]))[-1] + 1  for i in range(-12,13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_scale_rank = pd.DataFrame(correct_scale_rank, columns = ['Correct Scale Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interval recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = ['-octave',\n",
    " '-maj7',\n",
    " '-min7',\n",
    " '-maj6',\n",
    " '-min6',\n",
    " '-p5',\n",
    " '-tritone',\n",
    " '-p4',\n",
    " '-maj3',\n",
    " '-min3',\n",
    " '-maj2',\n",
    " '-min2',\n",
    " 'unison',\n",
    " 'min2',\n",
    " 'maj2',\n",
    " 'min3',\n",
    " 'maj3',\n",
    " 'p4',\n",
    " 'tritone',\n",
    " 'p5',\n",
    " 'min6',\n",
    " 'maj6',\n",
    " 'min7',\n",
    " 'maj7',\n",
    " 'octave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n",
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n",
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n",
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n",
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n"
     ]
    }
   ],
   "source": [
    "# for perm in range(1,6):\n",
    "    \n",
    "#     generator = IntervalRecognitionGenerator(output_dir = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/interval/perm' + str(perm) + '/')\n",
    "\n",
    "#     final_results = defaultdict(list)\n",
    "\n",
    "#     for interval in intervals:\n",
    "\n",
    "#         results = {key: [] for key in intervals}\n",
    "\n",
    "#         tests = generator.generate_comprehensive_test(interval, num_examples=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interval_recognition_benchmark(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results = False, n_perm=5):\n",
    "    \n",
    "    '''\n",
    "    Runs interval recognition benchmark.\n",
    "    Arguments:\n",
    "    get_mean_nll: callable, str -> float, takes in midi file and outputs mean NLL\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory\n",
    "    return_all_results: if True, return the percentile of the tested for every interval and permutation. If False, take the average over permutations and intervals and just return this value. Default=False\n",
    "    n_perm: set the number of times the experiment is repeated and averaged over. Must be >1. \n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of percentile of the tested interval for each interval (-octave to octave) for each permutation.\n",
    "    mean_percentile: Average percentile of tonic across all keys. Higher = better.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    intervals = ['-octave',\n",
    "     '-maj7',\n",
    "     '-min7',\n",
    "     '-maj6',\n",
    "     '-min6',\n",
    "     '-p5',\n",
    "     '-tritone',\n",
    "     '-p4',\n",
    "     '-maj3',\n",
    "     '-min3',\n",
    "     '-maj2',\n",
    "     '-min2',\n",
    "     'unison',\n",
    "     'min2',\n",
    "     'maj2',\n",
    "     'min3',\n",
    "     'maj3',\n",
    "     'p4',\n",
    "     'tritone',\n",
    "     'p5',\n",
    "     'min6',\n",
    "     'maj6',\n",
    "     'min7',\n",
    "     'maj7',\n",
    "     'octave']\n",
    "\n",
    "    final_results = defaultdict(list)\n",
    "\n",
    "    n_perm = n_perm\n",
    "\n",
    "    print(\"Testing interval...\")\n",
    "    for interval in intervals:\n",
    "\n",
    "        print(interval)\n",
    "        results = {key: [] for key in intervals}\n",
    "\n",
    "        for i in range(1, n_perm+1):\n",
    "            #print(i)\n",
    "            tests = glob.glob(PATH_TO_BENCHMARK_SUITE + 'interval/perm' + str(i) + '/' + interval + '*')\n",
    "            test_intervals = [x.split('.mid')[0].split('_')[-1] for x in tests]\n",
    "            tests_dict = dict(zip(test_intervals, tests))\n",
    "\n",
    "            keys = []\n",
    "            nlls = []\n",
    "\n",
    "            for key, value in tests_dict.items():\n",
    "                keys.append(key)\n",
    "                nlls.append(get_mean_nll(value))\n",
    "\n",
    "            min_nll = np.min(nlls)\n",
    "            nlls = np.array(nlls) - min_nll#\n",
    "\n",
    "            for j, x in enumerate(keys):\n",
    "                if x == 'correct':\n",
    "                    results[interval].append(nlls[j])\n",
    "                else:\n",
    "                    results[x].append(nlls[j])\n",
    "\n",
    "        target_relative_nlls = pd.DataFrame(results).T.rank(axis=0).T[interval]\n",
    "\n",
    "        final_results[interval] = list(target_relative_nlls.values.flatten())\n",
    "\n",
    "    final_results_df = pd.DataFrame(final_results, index = ['Permutation ' + str(x) for x in range(1,n_perm+1)])#.melt(var_name='Interval', value_name = 'Rank')['Rank'].mean()\n",
    "    final_results_df = ((24 - final_results_df) + 1)/24\n",
    "\n",
    "    mean_percentile = final_results_df.mean().mean()\n",
    "\n",
    "    \n",
    "    if return_all_results == True:\n",
    "        return final_results_df, mean_percentile\n",
    "    \n",
    "    else:\n",
    "        return mean_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing interval...\n",
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n"
     ]
    }
   ],
   "source": [
    "res, mean_percentile = run_interval_recognition_benchmark(get_mean_nll, PATH_TO_BENCHMARK_SUITE=PATH_TO_BENCHMARK_SUITE, return_all_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-octave\n",
      "-maj7\n",
      "-min7\n",
      "-maj6\n",
      "-min6\n",
      "-p5\n",
      "-tritone\n",
      "-p4\n",
      "-maj3\n",
      "-min3\n",
      "-maj2\n",
      "-min2\n",
      "unison\n",
      "min2\n",
      "maj2\n",
      "min3\n",
      "maj3\n",
      "p4\n",
      "tritone\n",
      "p5\n",
      "min6\n",
      "maj6\n",
      "min7\n",
      "maj7\n",
      "octave\n"
     ]
    }
   ],
   "source": [
    "# INTERVAL PREDICTION\n",
    "\n",
    "generator = IntervalRecognitionGenerator(output_dir = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/interval_examples/')\n",
    "\n",
    "final_results = defaultdict(list)\n",
    "\n",
    "for interval in intervals:\n",
    "    \n",
    "    print(interval)\n",
    "    results = {key: [] for key in intervals}\n",
    "\n",
    "    n_perm = 2\n",
    "\n",
    "    for i in range(n_perm):\n",
    "\n",
    "        # Generate comprehensive test for interval\n",
    "        tests = generator.generate_comprehensive_test(interval, num_examples=10)\n",
    "\n",
    "        tokenizer = TSDTokenizer()\n",
    "        \n",
    "        keys = []\n",
    "        nlls = []\n",
    "        \n",
    "        for key, value in tests.items():\n",
    "            keys.append(key)\n",
    "            nlls.append(get_mean_nll(lstm, TSDTokenizer(), value))\n",
    "            \n",
    "        min_nll = np.min(nlls)\n",
    "        nlls = np.array(nlls) - min_nll#\n",
    "\n",
    "        for j, x in enumerate(keys):\n",
    "            results[x].append(nlls[j])\n",
    "\n",
    "    target_relative_nlls = pd.DataFrame(results).T.rank(axis=0).T[interval]\n",
    "        \n",
    "    final_results[interval] = list(target_relative_nlls.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSPOSITION INVARIANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the generator\n",
    "# generator = IntervalLikelihoodGenerator(output_dir='/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/transposition/')\n",
    "\n",
    "# # Generate all intervals from C4 (MIDI note 60)\n",
    "# c4_tests = generator.generate_all_intervals(start_note=60)\n",
    "# print(f\"Generated {len(c4_tests)} interval tests from C4\")\n",
    "\n",
    "# notes = list(range(60-24, 60+25))\n",
    "# results = {key: [] for key in notes}\n",
    "\n",
    "# for note in notes:\n",
    "#     # Generate comprehensive test for perfect fifth\n",
    "#     tests = generator.generate_all_intervals(start_note=note)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transposition_invariance(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results = False):\n",
    "    \n",
    "    '''\n",
    "    Runs transpostion invariance benchmark.\n",
    "    Arguments:\n",
    "    get_mean_nll: callable, str -> float, takes in midi file and outputs mean NLL\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory\n",
    "    return_all_results: if True, return the likelihood of all intervals for every starting note two octaves above and below middle C. If False, just return the mean of the correlation between likelihood vectors of adjacent notes. Default=False\n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of the likelihood of all intervals for every starting note two octaves above and below middle C. \n",
    "    mean_percentile: Average correlation between interval likelihood vectors for adjacent notes.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    notes = list(range(60-24, 60+25))\n",
    "    results = {key: [] for key in notes}\n",
    "\n",
    "    intervals = ['neg_octave',\n",
    "     'neg_maj7',\n",
    "     'neg_min7',\n",
    "     'neg_maj6',\n",
    "     'neg_min6',\n",
    "     'neg_p5',\n",
    "     'neg_tritone',\n",
    "     'neg_p4',\n",
    "     'neg_maj3',\n",
    "     'neg_min3',\n",
    "     'neg_maj2',\n",
    "     'neg_min2',\n",
    "     'unison',\n",
    "     'min2',\n",
    "     'maj2',\n",
    "     'min3',\n",
    "     'maj3',\n",
    "     'p4',\n",
    "     'tritone',\n",
    "     'p5',\n",
    "     'min6',\n",
    "     'maj6',\n",
    "     'min7',\n",
    "     'maj7',\n",
    "     'octave']\n",
    "\n",
    "    for note in notes:\n",
    "\n",
    "        for interval in intervals:\n",
    "            fname = PATH_TO_BENCHMARK_SUITE + 'transposition/' + 'pitch' + str(note) + '_' + interval + '.mid'\n",
    "            results[note].append(get_mean_nll(fname))\n",
    "            \n",
    "    all_results = pd.DataFrame(results)\n",
    "\n",
    "    #transposition_invariance = all_results.corr().values[np.triu_indices(24, k= 1)].mean()\n",
    "\n",
    "    ix_adjacent_notes = np.vstack((np.arange(0,len(notes)-1),np.arange(1,len(notes))))#[:-1,:]\n",
    "    ix_adjacent_notes = tuple(ix_adjacent_notes)     \n",
    "    \n",
    "    invariance = all_results.corr().values[ix_adjacent_notes]\n",
    "    \n",
    "    \n",
    "    if return_all_results == True:\n",
    "        return all_results, invariance\n",
    "    \n",
    "    else:\n",
    "        return np.mean(invariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519195675494432"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_transposition_invariance(get_mean_nll, PATH_TO_BENCHMARK_SUITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "generator = IntervalLikelihoodGenerator(output_dir='/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/transposition/')\n",
    "\n",
    "# Generate all intervals from C4 (MIDI note 60)\n",
    "c4_tests = generator.generate_all_intervals(start_note=60)\n",
    "print(f\"Generated {len(c4_tests)} interval tests from C4\")\n",
    "\n",
    "notes = list(range(60-24, 60+25))\n",
    "results = {key: [] for key in notes}\n",
    "\n",
    "for note in notes:\n",
    "    print(note)\n",
    "    # Generate comprehensive test for perfect fifth\n",
    "    tests = generator.generate_all_intervals(start_note=note)\n",
    "\n",
    "    tokenizer = TSDTokenizer()\n",
    "\n",
    "    # Tokenize both MIDI files\n",
    "    tokens = [(key, tokenizer.tokenize(time_dilate_midi(input_path=value, output_path = value, scale_factor=1))) for key, value in tests.items()]\n",
    "    #tokens = [(key, tokenizer.tokenize(value)) for key, value in tests.items()]\n",
    "\n",
    "#     # Get model predictions\n",
    "#     predictions = [(key, lstm_small.predict_sequence(x)) for (key, x) in tokens]\n",
    "    \n",
    "#     for (key, x) in predictions:\n",
    "#         results[note].append(np.mean([nll for nll in x.nlls if nll is not None]))\n",
    "\n",
    "    for key, value in tests.items():\n",
    "        results[note].append(get_mean_nll(value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame(results)\n",
    "\n",
    "#transposition_invariance = all_results.corr().values[np.triu_indices(24, k= 1)].mean()\n",
    "\n",
    "ix_adjacent_notes = np.vstack((np.arange(0,len(notes)-1),np.arange(1,len(notes))))#[:-1,:]\n",
    "ix_adjacent_notes = tuple(ix_adjacent_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "invariance = all_results.corr().values[ix_adjacent_notes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5459220773146873"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(invariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_melody_comparison(get_mean_nll, PATH_TO_BENCHMARK_SUITE, human_ranking_data_loc = None, return_all_results = False):\n",
    "        \n",
    "    '''\n",
    "    Runs human melodic alignment benchmark.\n",
    "    Arguments:\n",
    "    get_mean_nll: callable, str -> float, takes in midi file and outputs mean NLL\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory\n",
    "    human_ranking_data_loc: location of Lokyan_t_01_human_processed_ratings.csv, with the human ratings data. \n",
    "    return_all_results: if True, return the correlation between model and human ratings for every context interval and for both keys (C and F#). Default=False\n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of the correlation between model and human ratings for every context interval and for both keys. \n",
    "    mean_correlation: Average correlation between model and human ratings across all contexts and both keys. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if human_ranking_data_loc==None:\n",
    "        human_ranking_data_loc = PATH_TO_BENCHMARK_SUITE + '/melody_continuation/Lokyan_t_01_human_processed_ratings.csv'\n",
    "        \n",
    "    context_intervals = ['M2_a', 'M2_d', 'M6_a', 'M6_d', 'm3_a', 'm3_d', 'm7_a', 'm7_d']\n",
    "    ending_notes = [\"C4\",'F#4']\n",
    "        #f_sharp_results = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "    c_results = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "\n",
    "    for interval in context_intervals:\n",
    "\n",
    "        results = {key: [] for key in range(1,26)}\n",
    "\n",
    "        for i in range(1,26):\n",
    "            fname = glob.glob(PATH_TO_BENCHMARK_SUITE + '/melody_continuation/' + interval + '_from_C4_cont_'+ str(i) + '_*.mid')[0]\n",
    "            results[i].append(get_mean_nll(fname))#np.mean([nll for nll in x.nlls if nll is not None]))\n",
    "\n",
    "        results = pd.DataFrame(results, index = [interval]).T\n",
    "\n",
    "        c_results[interval] = results[interval]\n",
    "\n",
    "\n",
    "    f_sharp_results = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "\n",
    "    for interval in context_intervals:\n",
    "\n",
    "        results = {key: [] for key in range(1,26)}\n",
    "        \n",
    "        for i in range(1,26):\n",
    "            fname = glob.glob(PATH_TO_BENCHMARK_SUITE + '/melody_continuation/' + interval + '_from_F#4_cont_'+ str(i) + '_*.mid')[0]\n",
    "            results[i].append(get_mean_nll(fname))#np.mean([nll for nll in x.nlls if nll is not None]))\n",
    "\n",
    "        results = pd.DataFrame(results, index = [interval]).T\n",
    "\n",
    "        f_sharp_results[interval] = results[interval]\n",
    "        \n",
    "    human_results = pd.read_csv(human_ranking_data_loc, index_col = 0)\n",
    "    human_results_reformatted = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "    for x, dat in human_results.groupby('stim_identity'):\n",
    "        human_results_reformatted[x] = dat['mean_rating'].values\n",
    "\n",
    "    model_human_corrs = pd.DataFrame(index = context_intervals, columns = ending_notes  + ['average'])\n",
    "\n",
    "    c_results_norm = (c_results-c_results.mean())/c_results.std()\n",
    "    f_sharp_results_norm = (f_sharp_results-f_sharp_results.mean())/f_sharp_results.std()\n",
    "\n",
    "    for interval in context_intervals:\n",
    "        model_human_corrs.at[interval, 'C4'] = stats.spearmanr(c_results[interval], -human_results_reformatted[interval])[0]\n",
    "        model_human_corrs.at[interval, 'F#4'] = stats.spearmanr(f_sharp_results[interval], -human_results_reformatted[interval])[0]\n",
    "        model_human_corrs.at[interval, 'average'] = stats.spearmanr(((c_results_norm + f_sharp_results_norm)/2)[interval], -human_results_reformatted[interval])[0]\n",
    "        \n",
    "    mean_correlation = np.mean(model_human_corrs['average'])\n",
    "    \n",
    "    if return_all_results == True:\n",
    "        return model_human_corrs, mean_correlation\n",
    "    \n",
    "    else:\n",
    "        return mean_correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7545058303704535"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_melody_comparison(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/../human_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/benchmark_suite/../human_data'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_BENCHMARK_SUITE + '../human_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN T01 - MELODY CONTINUATION\n",
    "generator = MelodyContinuationGenerator(output_dir='/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/examples/melody_continuation_examples/')\n",
    "context_dir = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/human_data/Stimuli/t_01/'\n",
    "ending_notes = ['C4','F#4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M2_a\n",
      "M2_d\n",
      "M6_a\n",
      "M6_d\n",
      "m3_a\n",
      "m3_d\n",
      "m7_a\n",
      "m7_d\n",
      "M2_a\n",
      "M2_d\n",
      "M6_a\n",
      "M6_d\n",
      "m3_a\n",
      "m3_d\n",
      "m7_a\n",
      "m7_d\n"
     ]
    }
   ],
   "source": [
    "context_intervals = ['M2_a', 'M2_d', 'M6_a', 'M6_d', 'm3_a', 'm3_d', 'm7_a', 'm7_d']\n",
    "\n",
    "#f_sharp_results = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "c_results = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "\n",
    "for interval in context_intervals:\n",
    "    print(interval)\n",
    "    continuations = generator.generate_continuations(context_midi_path=context_dir + 'C4/' + interval + '.mid', context_end_note=ending_notes[0], last_note_duration=480//2)\n",
    "\n",
    "    results = {key: [] for key in range(1,26)}\n",
    "#     tokenizer = TSDTokenizer()\n",
    "\n",
    "#     # Tokenize both MIDI files\n",
    "#     tokens = [(key, tokenizer.tokenize(value)) for key, value in continuations.items()]\n",
    "#     #tokens = [(key, tokenizer.tokenize(value)) for key, value in tests.items()]\n",
    "\n",
    "#     # Get model predictions\n",
    "#     predictions = [(key, lstm.predict_sequence(x)) for (key, x) in tokens]\n",
    "\n",
    "    for  key, value in continuations.items():\n",
    "        results[key].append(get_mean_nll(lstm, TSDTokenizer(), value))#np.mean([nll for nll in x.nlls if nll is not None]))\n",
    "\n",
    "    results = pd.DataFrame(results, index = [interval]).T\n",
    "    \n",
    "    c_results[interval] = results[interval]\n",
    "    \n",
    "    \n",
    "f_sharp_results = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "\n",
    "for interval in context_intervals:\n",
    "    print(interval)\n",
    "    continuations = generator.generate_continuations(context_midi_path=context_dir + 'F_sharp_4/' + interval + '.mid', context_end_note=ending_notes[1], last_note_duration=480//2)\n",
    "\n",
    "    results = {key: [] for key in range(1,26)}\n",
    "#     tokenizer = TSDTokenizer()\n",
    "\n",
    "#     # Tokenize both MIDI files\n",
    "#     tokens = [(key, tokenizer.tokenize(value)) for key, value in continuations.items()]\n",
    "#     #tokens = [(key, tokenizer.tokenize(value)) for key, value in tests.items()]\n",
    "\n",
    "#     # Get model predictions\n",
    "#     predictions = [(key, lstm.predict_sequence(x)) for (key, x) in tokens]\n",
    "\n",
    "    for  key, value in continuations.items():\n",
    "        results[key].append(get_mean_nll(lstm, TSDTokenizer(), value))\n",
    "\n",
    "    results = pd.DataFrame(results, index = [interval]).T\n",
    "    \n",
    "    f_sharp_results[interval] = results[interval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_results = pd.read_csv('/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/human_data/Lokyan_t_01_human_processed_ratings.csv', index_col = 0)\n",
    "human_results_reformatted = pd.DataFrame(index = range(1,26), columns = context_intervals)\n",
    "for x, dat in human_results.groupby('stim_identity'):\n",
    "    human_results_reformatted[x] = dat['mean_rating'].values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_human_corrs = pd.DataFrame(index = context_intervals, columns = ending_notes  + ['average'])\n",
    "\n",
    "c_results_norm = (c_results-c_results.mean())/c_results.std()\n",
    "f_sharp_results_norm = (f_sharp_results-f_sharp_results.mean())/f_sharp_results.std()\n",
    "\n",
    "for interval in context_intervals:\n",
    "    model_human_corrs.at[interval, 'C4'] = stats.spearmanr(c_results[interval], -human_results_reformatted[interval])[0]\n",
    "    model_human_corrs.at[interval, 'F#4'] = stats.spearmanr(f_sharp_results[interval], -human_results_reformatted[interval])[0]\n",
    "    model_human_corrs.at[interval, 'average'] = stats.spearmanr(((c_results_norm + f_sharp_results_norm)/2)[interval], -human_results_reformatted[interval])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_chord_comparison(get_mean_nll, PATH_TO_BENCHMARK_SUITE, human_ranking_data_loc = None, return_all_results = False):\n",
    "        \n",
    "    '''\n",
    "    Runs human chord alignment benchmark prediction benchmark. The stimuli were from SOURCE (Lokyan). \n",
    "    Arguments:\n",
    "    get_mean_nll: callable, str -> float, takes in midi file and outputs mean NLL\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory\n",
    "    human_ranking_data_loc: location of Lokyan_t_03_human_processed_ratings.csv, with the human ratings data. \n",
    "    return_all_results: if True, return the correlation between model and human ratings for every chord variants. Default=False\n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of the correlation between model and human ratings for  every chord variants. \n",
    "    mean_correlation: Average correlation between model and human ratings across all chord variants. \n",
    "    \n",
    "    '''\n",
    "    from scipy import stats\n",
    "    #NOTE: THIS INCLUDES NO TIME DILATION. PERFORMANCE MAY IMPROVE IF AVERAGING MODEL PERFORMANCE ACROSS SEVERAL TIME DILATION FACTORS.\n",
    "    \n",
    "    if human_ranking_data_loc==None:\n",
    "        human_ranking_data_loc = PATH_TO_BENCHMARK_SUITE + 'chord_alignment/Lokyan_t_03_human_processed_ratings.csv'\n",
    "        \n",
    "    human_results_t03 = pd.read_csv(human_ranking_data_loc, index_col = 0)\n",
    "    \n",
    "    chord_rating_path = PATH_TO_BENCHMARK_SUITE + 'chord_alignment/'\n",
    "    \n",
    "    results_human = pd.DataFrame(human_results_t03[['mean_ratings']].values.flatten(), index = human_results_t03['full_chord_names'].values.flatten(), columns = ['mean_ratings']) #{key: None for key in human_results_t03['full_chord_names'].values}#dict.fromkeys(human_results_t03['full_chord_names'].values)\n",
    "\n",
    "    results_model = pd.DataFrame(index = human_results_t03['full_chord_names'].values.flatten(), columns = ['model_mean_NLL'])#['model_mean_NLL'] = [np.nan]*len(results)\n",
    "\n",
    "    for x in human_results_t03['full_chord_names'].values:\n",
    "\n",
    "        fname = chord_rating_path + x + '.mid'\n",
    "\n",
    "        nll = get_mean_nll(fname)\n",
    "\n",
    "        results_model.at[x, 'model_mean_NLL'] = nll\n",
    "        \n",
    "    results_all = results_model.copy()\n",
    "    results_all['human_ratings'] = results_human['mean_ratings']\n",
    "    \n",
    "    correlation = stats.pearsonr(results_all['human_ratings'].values.flatten().astype(float), results_all['model_mean_NLL'].values.flatten().astype(float))[0]\n",
    "    \n",
    "    if return_all_results == True:\n",
    "        return results_all, correlation\n",
    "    \n",
    "    else:\n",
    "        return mean_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUMAN T03 - harmony ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_results_t03 = pd.read_csv('/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/human_data//Lokyan_t_03_human_processed_ratings.csv', index_col = 0)\n",
    "chord_rating_path = '/Users/IsaacSebenius/Dropbox/CambridgePhD/ExpectationProject/CognitiveBenchmarking/data/human_data/Stimuli/t_03/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bdim_diminished\n",
      "a_minor\n",
      "G_major\n",
      "F_major\n",
      "e_minor\n",
      "d_minor\n",
      "C_major\n",
      "C#_major\n",
      "D_major\n",
      "D#_major\n",
      "E_major\n",
      "F#_major\n",
      "Ab_major\n",
      "A_major\n",
      "Bb_major\n",
      "B_major\n",
      "c_minor\n",
      "c#_minor\n",
      "d#_minor\n",
      "f_minor\n",
      "f#_minor\n",
      "g_minor\n",
      "ab_minor\n",
      "bb_minor\n",
      "b_minor\n",
      "C7_seventh\n",
      "C#7_seventh\n",
      "D7_seventh\n",
      "D#7_seventh\n",
      "E7_seventh\n",
      "F7_seventh\n",
      "F#7_seventh\n",
      "G7_seventh\n",
      "Ab7_seventh\n",
      "A7_seventh\n",
      "Bb7_seventh\n",
      "B7_seventh\n",
      "c7_minor_seventh\n",
      "c#7_minor_seventh\n",
      "d7_minor_seventh\n",
      "eb7_minor_seventh\n",
      "e7_minor_seventh\n",
      "f7_minor_seventh\n",
      "f#7_minor_seventh\n",
      "g7_minor_seventh\n",
      "ab7_minor_seventh\n",
      "a7_minor_seventh\n",
      "bb7_minor_seventh\n",
      "b7_minor_seventh\n"
     ]
    }
   ],
   "source": [
    "results_human = pd.DataFrame(human_results_t03[['mean_ratings']].values.flatten(), index = human_results_t03['full_chord_names'].values.flatten(), columns = ['mean_ratings']) #{key: None for key in human_results_t03['full_chord_names'].values}#dict.fromkeys(human_results_t03['full_chord_names'].values)\n",
    "\n",
    "time_dilate_marginalization = [0.5,0.75,1,1.5,2]\n",
    "\n",
    "results_model = pd.DataFrame(index = human_results_t03['full_chord_names'].values.flatten(), columns = ['model_mean_NLL_dilation_' + str(x) for x in time_dilate_marginalization])#['model_mean_NLL'] = [np.nan]*len(results)\n",
    "\n",
    "for x in human_results_t03['full_chord_names'].values:\n",
    "    #nll = 0\n",
    "    print(x)\n",
    "    for dilation in time_dilate_marginalization:\n",
    "        \n",
    "        test_file = chord_rating_path + x + '.mid'\n",
    "        test_file = time_dilate_midi(input_path=test_file, output_path = 'temp.mid', scale_factor=dilation)\n",
    "        tokenizer = TSDTokenizer()\n",
    "        temp_nll = get_mean_nll(lstm, TSDTokenizer(), test_file)\n",
    "        #tokens = tokenizer.tokenize(test_file)\n",
    "        #predictions = lstm.predict_sequence(tokens)\n",
    "        #temp_nll = np.mean([nll for nll in predictions.nlls if nll is not None])\n",
    "        results_model.at[x, 'model_mean_NLL_dilation_' + str(dilation)] = temp_nll #(temp_nll)\n",
    "    #nll /= len(time_dilate_marginalization)\n",
    "    #results[x] = [nll]\n",
    "    #results.at[x, 'model_mean_NLL'] = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_model = ((results_model - results_model.mean())/results_model.std())#.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.465204081632653, pvalue=0.0007571889349679405)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(results_model.mean(1).values.flatten().astype(float), results_human['mean_ratings'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.14795918367346936, pvalue=0.3103085094179323)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(results_model['model_mean_NLL_dilation_1.5'].values.flatten().astype(float), results_human['mean_ratings'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glass comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_glass_benchmark(times, surprisals, PATH_TO_BENCHMARK_SUITE, human_ratings_loc=None,  return_all_results = False):\n",
    "    \n",
    "    '''\n",
    "    Calculates human-model correlation between surprisal at various points in Glass's The Hours (from glass.mid in the benchmark_suite/glass/ directory).\n",
    "    \n",
    "    Arguments:\n",
    "    times: np.array, vector of timestamps (in seconds) where model surprisal is estimated. The more fine-grained the time is (e.g. the more samples), closer they can be aligned to the timepoints of human ratings. \n",
    "    surprisals: np.array, vector of model surprisal estimates\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory.\n",
    "    human_ratings_loc: str, the location of the file \"GlassEvents.txt\" By default this is in the \"glass\" folder within the benchmark_suite directory. \n",
    "    return_all_results: Bool, whether to return the surprisal at each of the musical events where there are human ratings. Otherwise just return the correlation. Default = False\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of the correlation between model and human ratings for  every chord variants. \n",
    "    mean_correlation: Average correlation between model and human ratings across all chord variants. \n",
    "    \n",
    "    '''\n",
    "        \n",
    "    if human_ratings_loc==None:\n",
    "        human_ratings_loc = PATH_TO_BENCHMARK_SUITE + 'glass/GlassEvents.txt'\n",
    "        \n",
    "    surprise_events = pd.read_csv(human_ratings_loc)\n",
    "    surprise_events['Group'] = ['HS']*17 + ['LS']*17 + ['US']*17\n",
    "    surprise_events = surprise_events.fillna(0)\n",
    "    \n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx, array[idx]\n",
    "    \n",
    "    model_surprisal = [surprisals[find_nearest(times, onset)[0]] for onset in surprise_events['Onset'].values.flatten()]\n",
    "    surprise_events['Model surprisal'] = model_surprisal\n",
    "    \n",
    "    correlation = stats.spearmanr(surprise_events['Rank'], model_surprisal)[0]\n",
    "    \n",
    "        \n",
    "    if return_all_results == True:\n",
    "        return surprise_events, correlation\n",
    "    \n",
    "    else:\n",
    "        return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mussorgsky comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mussosgsky_benchmark(times, surprisals, PATH_TO_BENCHMARK_SUITE, human_ratings_loc=None,  return_all_results = False):\n",
    "    \n",
    "    '''\n",
    "    Calculates human-model correlation between surprisal at various points in Mussorgsky's Night on Bald Mountain (from nobm.mid in the benchmark_suite/mussorgsky/ directory).\n",
    "    \n",
    "    Arguments:\n",
    "    times: np.array, vector of timestamps (in seconds) where model surprisal is estimated. The more fine-grained the time is (e.g. the more samples), closer they can be aligned to the timepoints of human ratings. \n",
    "    surprisals: np.array, vector of model surprisal estimates.\n",
    "    PATH_TO_BENCHMARK_SUITE: str, path to benchmark_suite directory.\n",
    "    human_ratings_loc: str, the location of the file \"MussorgskyEvents.txt\" By default this is in the \"mussorgsky\" folder within the benchmark_suite directory. \n",
    "    return_all_results: Bool, whether to return the surprisal at each of the musical events where there are human ratings. Otherwise just return the correlation. Default = False\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    results: dataframe of the correlation between model and human ratings for  every chord variants. \n",
    "    mean_correlation: Average correlation between model and human ratings across all chord variants. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if human_ratings_loc==None:\n",
    "        human_ratings_loc = PATH_TO_BENCHMARK_SUITE + 'mussorgsky/MussorgskyEvents.txt'\n",
    "        \n",
    "    surprise_events = pd.read_csv(human_ratings_loc)\n",
    "    surprise_events['Group'] = ['HS']*28 + ['LS']*28 + ['US']*28\n",
    "    surprise_events = surprise_events.fillna(0)\n",
    "    \n",
    "    def find_nearest(array, value):\n",
    "        array = np.asarray(array)\n",
    "        idx = (np.abs(array - value)).argmin()\n",
    "        return idx, array[idx]\n",
    "    \n",
    "    model_surprisal = [surprisals[find_nearest(times, onset)[0]] for onset in surprise_events['Onset'].values.flatten()]\n",
    "    surprise_events['Model surprisal'] = model_surprisal\n",
    "    \n",
    "    correlation = stats.spearmanr(surprise_events['Rank'], model_surprisal)[0]\n",
    "    \n",
    "        \n",
    "    if return_all_results == True:\n",
    "        return surprise_events, correlation\n",
    "    \n",
    "    else:\n",
    "        return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass = pd.read_csv('../NLL_over_time_Glass_MT.csv', index_col = 0)\n",
    "times, surprisal = glass['Time (s)'].values.flatten(),  glass['NLL-moving sum over 1s window'].values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5053330096541302"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_glass_benchmark(times, surprisal, PATH_TO_BENCHMARK_SUITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "mussorgsky = pd.read_csv('../NLL_over_time_Mussorgsky_MT.csv', index_col = 0)\n",
    "times, surprisal = mussorgsky['Time (s)'].values.flatten(),  mussorgsky['NLL-moving sum over 1s window'].values.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4367548635254413"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_mussosgsky_benchmark(times, surprisal, PATH_TO_BENCHMARK_SUITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (music-env)",
   "language": "python",
   "name": "music-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
