{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample: Running the benchmarks\n",
    "\n",
    "This notebook shows how to run each of the eight benchmarks. Ensure the project root is on your path, load your model and tokenizer, and define the NLL callables. Each section below runs one benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: add project root to path and set benchmark suite location\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()  # or Path('/path/to/CognitiveBenchmarking')\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "PATH_TO_BENCHMARK_SUITE = PROJECT_ROOT / 'data' / 'benchmark_suite'\n",
    "PATH_TO_BENCHMARK_SUITE = str(PATH_TO_BENCHMARK_SUITE) + '/'  # scripts expect trailing slash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer (adjust model path as needed)\n",
    "import torch\n",
    "from amads.expectation.tokenizer import TSDTokenizer_Custom\n",
    "\n",
    "model_path = PROJECT_ROOT.parent / 'lstm_atepp_not_bad_quality_embedd23_hidden64_300epochs_batch4_lr0.01_tokenizer_fixed.pth'\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "model.eval()\n",
    "model.device = 'cpu'\n",
    "\n",
    "custom_params = {'time_range': (0.01, 1), 'time_factor': 1}\n",
    "tokenizer = TSDTokenizer_Custom(config_params=custom_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NLL callables for the benchmarks (used by 6 of the 8 benchmarks)\n",
    "def get_mean_nll(midi_path):\n",
    "    tokens = tokenizer.tokenize(midi_path)\n",
    "    predictions = model.predict_sequence(tokens)\n",
    "    nlls = [nll for nll in predictions.nlls if nll is not None]\n",
    "    return float(np.mean(nlls))\n",
    "\n",
    "def get_total_nll(midi_path):\n",
    "    tokens = tokenizer.tokenize(midi_path)\n",
    "    predictions = model.predict_sequence(tokens)\n",
    "    nlls = [nll for nll in predictions.nlls if nll is not None]\n",
    "    return float(np.sum(nlls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cadence prediction\n",
    "\n",
    "Mean percentile of tonic resolution across 12 keys (higher = better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cadence prediction — mean percentile: 0.8958333333333334\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import run_cadence_prediction_benchmark\n",
    "result = run_cadence_prediction_benchmark(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Cadence prediction — mean percentile:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scale filling\n",
    "\n",
    "Mean percentile of correct scale completion across keys (higher = better). Uses total NLL with control subtraction (total NLL of a scale in the absence of chord context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale filling — mean percentile: 0.7283333333333334\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import run_scale_filling_benchmark\n",
    "result = run_scale_filling_benchmark(get_total_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Scale filling — mean percentile:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interval recognition\n",
    "\n",
    "Mean percentile of correct interval across 25 interval types and 5 permutations (higher = better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interval recognition — mean percentile: 0.5766666666666667\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import run_interval_recognition_benchmark\n",
    "result = run_interval_recognition_benchmark(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False, n_perm=5)\n",
    "print('Interval recognition — mean percentile:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transposition invariance\n",
    "\n",
    "Mean correlation of interval NLL vectors between adjacent starting notes (higher = more invariant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposition invariance — mean adjacent-note correlation: 0.519195675494432\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import get_transposition_invariance\n",
    "result = get_transposition_invariance(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Transposition invariance — mean adjacent-note correlation:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human melody continuation (Lokyan t_01)\n",
    "\n",
    "Mean Spearman correlation between model NLL and human ratings for melody continuations (higher = better alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human melody continuation — mean correlation: 0.7545058303704535\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import human_melody_comparison\n",
    "result = human_melody_comparison(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Human melody continuation — mean correlation:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Human chord alignment (Lokyan t_03)\n",
    "\n",
    "Pearson correlation between model mean NLL and human harmony ratings (higher = better alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human chord alignment — Pearson correlation: 0.6867327876262146\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import human_chord_comparison\n",
    "result = human_chord_comparison(get_mean_nll, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Human chord alignment — Pearson correlation:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Glass (The Hours)\n",
    "\n",
    "Requires precomputed NLL over time for the Glass piece. Spearman correlation between model surprisal at event onsets and human surprise ranks (higher = better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glass benchmark — Spearman correlation: 0.5053330096541302\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from benchmarks import run_glass_benchmark\n",
    "\n",
    "# Load precomputed NLL over time (e.g. from your model on glass.mid)\n",
    "# Columns expected: 'Time (s)', 'NLL-moving sum over 1s window' (or equivalent)\n",
    "glass_csv = PROJECT_ROOT.parent / 'NLL_over_time_Glass_MT.csv'  # adjust path\n",
    "glass_df = pd.read_csv(glass_csv, index_col=0)\n",
    "times = glass_df['Time (s)'].values.flatten()\n",
    "surprisal = glass_df['NLL-moving sum over 1s window'].values.flatten()\n",
    "\n",
    "result = run_glass_benchmark(times, surprisal, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Glass benchmark — Spearman correlation:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mussorgsky (Night on Bald Mountain)\n",
    "\n",
    "Requires precomputed NLL over time for the piece. Spearman correlation between model surprisal at event onsets and human surprise ranks (higher = better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mussorgsky benchmark — Spearman correlation: 0.4367548635254413\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from benchmarks import run_mussorgsky_benchmark\n",
    "\n",
    "mussorgsky_csv = PROJECT_ROOT.parent / 'NLL_over_time_Mussorgsky_MT.csv'  # adjust path\n",
    "mussorgsky_df = pd.read_csv(mussorgsky_csv, index_col=0)\n",
    "times = mussorgsky_df['Time (s)'].values.flatten()\n",
    "surprisal = mussorgsky_df['NLL-moving sum over 1s window'].values.flatten()\n",
    "\n",
    "result = run_mussorgsky_benchmark(times, surprisal, PATH_TO_BENCHMARK_SUITE, return_all_results=False)\n",
    "print('Mussorgsky benchmark — Spearman correlation:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (music-env)",
   "language": "python",
   "name": "music-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
